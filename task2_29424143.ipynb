{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Assessment 1\n",
    "## TASK 2: Text Pre-processing\n",
    "#### Student Name: Harshavardhan Reddy Mallypally\n",
    "#### Student ID: 29424143"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date: 12/08/2018\n",
    "Juypter version 5.5.0\n",
    "\n",
    "Environment: Python 3.6.0 and Anaconda 5.2.0 (64-bit)\n",
    "\n",
    "Packages used:             \n",
    "                **regular expression**        \n",
    "                **nltk**               \n",
    "                **itertools**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Introduction\n",
    "This task is of analyzing textUal data, i.e., converting the extracted data into a proper format. Here, we written python code  to preprocess a set of resumes that are provided and convert them into numerical representations.\n",
    "\n",
    "Since, I can't run **pdfminer** I'm going to use text format resumes that shared in drive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.collections import *\n",
    "import itertools\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extracting Resumes\n",
    "## Approach to extract the resumes\n",
    "1. First of all, extracted the file numbers under my student ID from the <font color=\"blue\">resume_dataset.txt</font> file.\n",
    "2. Using file numbers open the resume files and regex <font color=\"blue\">r\"\\w+(?:[-']\\w+)?\"</font> given in the assignment extracted text from ach resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening resume_dataset.txt for extracting file_numbers\n",
    "with open('resume_dataset.txt','r') as myresume:\n",
    "    resume = myresume.read()\n",
    "    resume_num = re.findall(r\"29424143:\\S(.*?)(?s)[]]\",resume)\n",
    "temp1=(str(resume_num).replace('\\\\n','')).replace(' ','*')\n",
    "file_numbers = re.findall(r'(\\d{1,3})',str(temp1))\n",
    "myresume.close()\n",
    "\n",
    "# using given regex, resume files words are extracted \n",
    "token_vocab=[]\n",
    "for i in range(0,len(file_numbers)):\n",
    "    filename='resume_('+str(file_numbers[i])+').txt'\n",
    "    with open(filename,'r',encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "        vocab_regex = re.findall(r\"\\w+(?:[-']\\w+)?\",text)\n",
    "        token_vocab.append(vocab_regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Porter Stemmer\n",
    "\n",
    "Stemmer is used to find the stem word for each and every word from resume files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemmer is implimented on tokens\n",
    "stemmer = nltk.PorterStemmer()\n",
    "stemmed_list = []\n",
    "for filetxt in token_vocab:\n",
    "    wordlist = []\n",
    "    for word in filetxt:\n",
    "        new_word = stemmer.stem(word)\n",
    "        wordlist.append(word)\n",
    "    stemmed_list.append(wordlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stop Words Removal\n",
    "\n",
    "stopwords are given in stopwords_en.txt to remove stopwords for the words extracted for the resume files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords are extracted\n",
    "with open(\"stopwords_en.txt\",\"r\") as stopwd:\n",
    "    stopwords = stopwd.read()\n",
    "stopword = stopwords.split(\"\\n\")\n",
    "\n",
    "\n",
    "# stopwords are remove for resume words\n",
    "temp=[]\n",
    "stop_words=[]\n",
    "stop_rm_vocab=[]\n",
    "for stem_words in stemmed_list:\n",
    "    for word in stem_words:\n",
    "        if word not in stopword:\n",
    "            stop_words.append(word)\n",
    "            temp.append(word)\n",
    "    stop_rm_vocab.append(stop_words)\n",
    "    stop_words=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Removing Context Dependent and Context Independent \n",
    "\n",
    "removing the 98% threshold words and 2% rare words for creating the final vocabulary list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicates are removed for the resume words \n",
    "total_words= []\n",
    "for filetxt in stop_rm_vocab:\n",
    "    for word in filetxt:\n",
    "        if word not in total_words:\n",
    "            total_words.append(word)\n",
    "            \n",
    "# (98% and above) and (2& and below) threshold worlds are removed\n",
    "indep_word  = []\n",
    "for total in total_words:\n",
    "    count = 0\n",
    "    for stop_rm_vocab1 in stop_rm_vocab:\n",
    "        if total in stop_rm_vocab1:\n",
    "            count += 1\n",
    "    if count <245 and count>5:\n",
    "        indep_word.append(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Finding Bigrams\n",
    "Finding  the Bigrams using ntlk packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 200 bigrams are found\n",
    "lists=  []\n",
    "for st in stop_rm_vocab:\n",
    "    lists += st\n",
    "\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(st)\n",
    "bigram_finder.apply_freq_filter(0)\n",
    "bigram_finder.apply_word_filter(lambda w: len(w) < 3)\n",
    "top_200_bigrams = bigram_finder.nbest(bigram_measures.pmi, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Creating vocab text\n",
    "Creating vocab text containing both bigrams and unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab text is created to load bigrams and vocab words \n",
    "voc_file = open(\"29414243_vocab.txt\",\"w\",encoding='utf-8')\n",
    "\n",
    "#top 200 bigrams are added here\n",
    "voc_string = \"**********************************200 MEANINGFUL BIGRAMS*************************************\"\n",
    "voc_file.write(voc_string)\n",
    "for tuples in top_200_bigrams:\n",
    "    key,value = tuples\n",
    "    voc_file.write(str(key)+','+str(value)+'\\n')\n",
    "    \n",
    "# vocabs are added from here\n",
    "voc_string = \"**********************************VOCAB*************************************\"\n",
    "for word in indep_word:\n",
    "    voc_file.write(str(word)+':'+str(indep_word.index(word))+'\\n')\n",
    "voc_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Sparse representation\n",
    "Sparse Representation of words which containes the filename follwed by token index followed by the count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty list created\n",
    "countvec=[]\n",
    "# for loop is created to get the filname, token_index and count\n",
    "for filetxt in stop_rm_vocab:\n",
    "    count_voc = open(\"29424143_countvec.txt\",\"w\") \n",
    "    count=0\n",
    "    for word in indep_word:\n",
    "        tok_index= indep_word.index(word)\n",
    "        if word in filetxt:\n",
    "            count1 = filetxt.count(word)\n",
    "            countvec.append('resume_('+str(file_numbers[stop_rm_vocab.index(filetxt)])+').txt:'+str(tok_index)+':'+str(count1))    \n",
    "#countvect.txt file is opened in write mode\n",
    "count_voc = open(\"29424143_countvec.txt\",\"w\")    \n",
    "count_voc.write(\"\\n\".join(countvec))\n",
    "count_voc.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\* resume files are extracted                                        \n",
    "\\* stemmer applied on the words                         \n",
    "\\* Stop words are removed                                   \n",
    "\\* context dependency is also removed                        \n",
    "\\* top 200 bigrams are found                            \n",
    "\\* created vocab_text                                        \n",
    "\\* created Sparse representation                                        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
